# -*- coding: utf-8 -*-
"""329198_DSL project_Autumn

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17YyhympYemuukBfE8IK6FlWWOEUxlvG1

# Importing Necessary Libraries

Here, importing all the libraries necessary for our data analysis and machine learning tasks. Such are: libraries for data manipulation-pandas; Numerical operations and array handling- numpy;text manipulation- Natural Language Toolkit (nltk); Data visualization Matplotlib- seaborn; machine learning models- sklearn; Lemmatization for reducing words to their base form
"""

import nltk
nltk.download('stopwords')

import nltk
nltk.download('wordnet')

import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeClassifier
from sklearn.multioutput import MultiOutputRegressor
from sklearn.model_selection import train_test_split,GridSearchCV
from sklearn.metrics import make_scorer, mean_squared_error
from sklearn.metrics import f1_score, classification_report
import matplotlib.pyplot as plt
pd.set_option('display.max_rows', 100)
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from nltk.corpus import stopwords
import re
import string
from nltk.stem import WordNetLemmatizer

"""# 1: Project Overview
People express a range of sentiments using various words, especially on online social media platforms. Psychological studies indicate that sentiments and emotions exist on a broad spectrum and can be categorized into different taxonomies. In this project,  we are asked to predict the sentiment of Twitter content, which is simplified to either *positive* or *negative*.

# 2: Data Loading and Initial Setup
Here, configuring the environment to display a sufficient number of rows for data examination. Besides, mount the Google Drive to access the project dataset and load it into a pandas DataFrame for subsequent analysis.
"""

from google.colab import drive
drive.mount('/content/drive')

# maximum number of rows pandas display= 100; for better dataframe visibility
pd.set_option('display.max_rows', 100)

# Define the path to the dataset stored on Google Drive
development_data = '/content/drive/MyDrive/development.csv'
evaluation_data = '/content/drive/MyDrive/evaluation.csv'

# Load the dataset from the CSV file into a pandas DataFrame
df = pd.read_csv(development_data)
df_eval= pd.read_csv(evaluation_data)
# Display the first five rows of the dataframe to verify it loaded correctly
df.head(5)

"""#3. Data Preprocessing
This section prepares text data for sentiment analysis by initializing stopwords and a lemmatizer, then applying a function to clean the textâ€”removing URLs, punctuation, and stopwords while lemmatizing. It drops irrelevant columns and ensures the sentiment labels are in integer format, ready for model training.
"""

stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def preprocess_text(text):
    # Convert to lowercase
    text = text.lower()
    # Remove URLs
    text = re.sub(r"http\S+|www\S+|https\S+", '', text, flags=re.MULTILINE)
    # Remove punctuation
    text = text.translate(str.maketrans('', '', string.punctuation))
    # Tokenize and lemmatize
    tokens = [lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words]
    return ' '.join(tokens)

# Drop irrelevant columns
df=df.drop(['date', 'flag', 'user'], axis=1)
#evaluation_data = df_eval.drop(['date', 'flag', 'user'], axis=1)

# Preprocess the text column
df['text'] = df['text'].apply(preprocess_text)
df_eval['text'] = df_eval['text'].apply(preprocess_text)

# Ensure sentiment labels are in the correct format
df['sentiment'] = df['sentiment'].astype(int)

df.head(5)

# Display a concise summary of the DataFrame
df.info()

"""#A. Feature Engineering
In this part of data-preprocessing, transforming text into numerical features using TF-IDF, selecting the 1,500 most relevant words.
Next, for a good understanding enriched the DataFrame `df` by adding features like `word_count`, `char_count`, `avg_word_length`, and . It also computes a sentiment score using the Afinn lexicon `afinn_sentiment`, which is stored in new columns.
"""

tdf_idf_vectorizer=TfidfVectorizer(max_features=1500)

# Add word count feature
df['word_count'] = df['text'].apply(lambda x: len(x.split()))

# Add character count feature
df['char_count'] = df['text'].apply(lambda x: len(x))

# Add average word length feature
df['avg_word_length'] = df['text'].apply(lambda x: np.mean([len(word) for word in x.split()]))

pip install afinn

from afinn import Afinn

# Initialize Afinn
afinn = Afinn()

# Adding positive and negative word sentiment score feature
df['afinn_sentiment'] = df['text'].apply(lambda x: afinn.score(x))
df.head(2)

"""#B. Statistical and visual analysis of sentiment data

Sentiment Distribution: This visualization illustrates the proportion of positive and negative sentiments within the dataset. Assessing the overall sentiment balance and detecting any imbalances that could impact model performance.
"""

# Plotting sentiment distribution
plt.figure(figsize=(8, 5))
sns.countplot(x='sentiment', data=df)
plt.title('Sentiment Distribution')
plt.xlabel('Sentiment')
plt.ylabel('Count')
plt.xticks(ticks=[0, 1], labels=['Negative', 'Positive'])
plt.show()

"""Word Cloud: This graphic highlights the most common words linked to positive-negative sentiments. showcases key terms, offering insights and connections into the language used in."""

from wordcloud import WordCloud

# word cloud for positive sentiments
positive_text = ' '.join(df[df['sentiment'] == 1]['text'])
wordcloud_positive = WordCloud(width=800, height=400, background_color='white').generate(positive_text)

# Plotting the positive word cloud
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud_positive, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud for Positive Sentiments')
plt.show()

# word cloud for negative sentiments
negative_text = ' '.join(df[df['sentiment'] == 0]['text'])
wordcloud_negative = WordCloud(width=800, height=400, background_color='black').generate(negative_text)

# Plotting the negative word cloud
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud_negative, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud for Negative Sentiments')
plt.show()

"""Correlation Heatmap: Displays the relationships among numeric features in the dataset, including text length features and sentiment scores. Showing how various features might affect sentiment classification."""

#only numeric columns for correlation
numeric_df = df.select_dtypes(include=[np.number])
# Calculating correlations
correlation_matrix = numeric_df.corr()

# Plotting the heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Heatmap')
plt.show()

"""#4. Model Development
for the model setup, divided the dataset into training and evaluation sets, then trained a classification pipeline using an MLPClassifier with TF-IDF vectorization. It makes predictions on the evaluation set and computes the F1 macro score to evaluate model performance.
"""

# Splitting the data into training and evaluation sets
X_train, X_test, y_train, y_test = train_test_split(df['text'], df['sentiment'], test_size=0.2, random_state=42)

"""Setting up MLPClassifier"""

from sklearn.neural_network import MLPClassifier

# Defining TF-IDF vectorizer
tfidf_vectorizer = TfidfVectorizer()

#the pipeline with MLPClassifier
pipeline = Pipeline([
    ('tfidf', tfidf_vectorizer),
    ('clf', MLPClassifier(hidden_layer_sizes=(100,), activation='relu',
                                       solver='adam', alpha=0.0001, batch_size='auto', random_state=42,
                                       verbose=True, early_stopping=True))
])

# Fit the pipeline on the training data
pipeline.fit(X_train, y_train)

# predicting on the evaluation set (X_test)
y_pred = pipeline.predict(X_test)

# Calculating the F1 macro score
f1_macro = f1_score(y_test, y_pred, average='macro')

# Print F1 macro score
print(f'F1 Macro Score: {f1_macro:.4f}')

# print detailed classification report
# print(classification_report(y_test, y_pred))

y_pred = np.round(y_pred).astype(int)

# Calculate accuracy
f1_macro = f1_score(y_test, y_pred, average='macro')
print(f'F1 Score: {f1_macro:.4f}')

"""In the below part, here it tune hyperparameters for an MLPClassifier using GridSearchCV with 3-fold cross-validation. It identifies the best parameters, uses the optimal model to predict sentiments on the evaluation set, calculates the F1 macro score, and displays a classification report detailing precision, recall, and F1 scores for each class."""

from sklearn.model_selection import GridSearchCV
from sklearn.neural_network import MLPClassifier
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import f1_score, classification_report

# Only the most important parameters for tuning
param_grid = {
    'clf__hidden_layer_sizes': [(100,), (50,)],
    'clf__activation': ['relu'],
    'clf__alpha': [0.0001],
    'clf__learning_rate': ['constant', 'adaptive'],
    'clf__batch_size': ['auto']
}

# Perform GridSearchCV with 3-fold cross-validation
grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='f1_macro', n_jobs=-1, verbose=1)

# Fit the grid search to the training data
grid_search.fit(X_train, y_train)

# Best parameters found by GridSearchCV
print(f"Best Parameters: {grid_search.best_params_}")

# Using the best model to make predictions on the evaluation set
best_model = grid_search.best_estimator_
y_pred_best = best_model.predict(X_test)

# Calculate F1 macro score with the best model
f1_macro = f1_score(y_test, y_pred_best, average='macro')

# F1 macro score
print(f'F1 Macro Score: {f1_macro:.4f}')

# printing classification report
print(classification_report(y_test, y_pred_best))

"""#5. Model Evaluation
In this sections, evaluated the model on new dataset- a key aspect of practical applicability. After that generating the final CSV file for submission and evaluation.
"""

# Predict on the evaluation dataset
y_eval = pipeline.predict(df_eval['text'])

y_pred

submission = pd.DataFrame({'Id':df_eval['ids'],'Predicted':y_eval})
submission.to_csv('/content/drive/MyDrive/submissionDSLmainf.csv',index=False,header=True)

"""#A. Visualization

Confusion Matrix: providing insights into how well the chosen model predicts each sentiment class.
"""

from sklearn.metrics import confusion_matrix
#confusion matrix
cm = confusion_matrix(y_test, y_pred_best)

# Plotting the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Negative', 'Positive'],
            yticklabels=['Negative', 'Positive'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

"""Precision-Recall Curve: shows and plotting the precision and recall at different thresholds,besides the average precision score for a comprehensive understanding of model performance."""

from sklearn.metrics import precision_recall_curve
from sklearn.metrics import average_precision_score

# Calculating precision, recall, and thresholds
y_scores = best_model.predict_proba(X_test)[:, 1]  # Getting probabilities
precision, recall, thresholds = precision_recall_curve(y_test, y_scores)

# average precision score
avg_precision = average_precision_score(y_test, y_scores)

# the Precision-Recall curve
plt.figure(figsize=(8, 6))
plt.plot(recall, precision, marker='.', label='Precision-Recall curve (AP = {:.2f})'.format(avg_precision))
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend()
plt.grid()
plt.show()

